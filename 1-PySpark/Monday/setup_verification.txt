1.  java -version
    openjdk version "17.0.18" 2026-01-20
    OpenJDK Runtime Environment (build 17.0.18+8-Ubuntu-124.04.1)
    OpenJDK 64-Bit Server VM (build 17.0.18+8-Ubuntu-124.04.1, mixed mode, sharing)

2.  WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
26/02/23 16:37:32 WARN Utils: Your hostname, DESKTOP-V633CFM, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
26/02/23 16:37:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 4.1.1
      /_/

Using Scala version 2.13.17, OpenJDK 64-Bit Server VM, 17.0.18
Branch HEAD
Compiled by user runner on 2026-01-02T11:55:02Z
Revision c0690c763bafabd08e7079d1137fa0a769a05bae
Url https://github.com/apache/spark

3.  >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda a, b: a + b)
    15

4. Spark failed to run because the spark_home enviroment variable and
    path configuration were missing. i resolved it by properly configuring system enviroment.